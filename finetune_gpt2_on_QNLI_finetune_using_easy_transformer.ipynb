{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from easy_transformer import EasyTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4427307",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "reference_gpt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in reference_gpt2.parameters())\n",
    "print(\"Number of parameters in GPT-2 Small model:\", num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('glue', 'qnli', split='train')\n",
    "validation_dataset = dataset_study = load_dataset('glue', 'qnli', split='validation')\n",
    "test_dataset = dataset_study = load_dataset('glue', 'qnli', split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 3000\n",
    "test_samples = 500 \n",
    "valid_samples = 500 \n",
    "subset_indices = torch.randperm(len(train_dataset)).tolist()[:num_samples]\n",
    "subset_dataset_train = train_dataset.select(subset_indices)\n",
    "\n",
    "subset_indices = torch.randperm(len(validation_dataset)).tolist()[:valid_samples]\n",
    "subset_dataset_valid = train_dataset.select(subset_indices)\n",
    "\n",
    "subset_indices = torch.randperm(len(test_dataset)).tolist()[:test_samples]\n",
    "subset_dataset_test = train_dataset.select(subset_indices)\n",
    "\n",
    "len(subset_dataset_train), len(subset_dataset_valid), len(subset_dataset_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = 0\n",
    "c1 = 0\n",
    "for each in subset_dataset_train:\n",
    "    if each['label'] == 0:\n",
    "        c0 += 1\n",
    "    if each['label'] == 1:\n",
    "        c1 += 1\n",
    "print(c0, c1)\n",
    "\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for each in subset_dataset_valid:\n",
    "    if each['label'] == 0:\n",
    "        c0 += 1\n",
    "    if each['label'] == 1:\n",
    "        c1 += 1\n",
    "print(c0, c1)\n",
    "\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for each in subset_dataset_test:\n",
    "    if each['label'] == 0:\n",
    "        c0 += 1\n",
    "    if each['label'] == 1:\n",
    "        c1 += 1\n",
    "print(c0, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0cdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ee7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for each in validation_dataset:\n",
    "    values.append(each['label'])\n",
    "set(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf37ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, model, max_length = 1024, token_to_add = 50256):\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "        self.token_to_add = token_to_add\n",
    "        self.model = model\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence1 = self.dataset[idx]['question']\n",
    "        sentence1_tokens = self.model.to_tokens(sentence1, prepend_bos = True)\n",
    "        sentence2 = self.dataset[idx]['sentence']\n",
    "        sentence2_tokens = self.model.to_tokens(sentence2, prepend_bos = True)\n",
    "        concatenated_tokens = torch.cat((sentence1_tokens, sentence2_tokens), dim=1)\n",
    "        labels = torch.tensor(self.dataset[idx]['label'])\n",
    "        \n",
    "        remaining_length = self.max_length - concatenated_tokens.size(1)\n",
    "        while remaining_length > 0:\n",
    "            concatenated_tokens = torch.cat((concatenated_tokens, torch.tensor([[self.token_to_add]])), dim=1)\n",
    "            remaining_length -= 1\n",
    "\n",
    "        \n",
    "        return concatenated_tokens, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(subset_dataset_train, reference_gpt2, max_length = 1024, token_to_add = 50256)\n",
    "data_loader_train = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "len(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(subset_dataset_valid, reference_gpt2, max_length = 1024, token_to_add = 50256)\n",
    "data_loader_valid = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "len(data_loader_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57763a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(subset_dataset_test, reference_gpt2, max_length = 1024, token_to_add = 50256)\n",
    "data_loader_test = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "len(data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens, labels in data_loader_train:\n",
    "    print(\"Tokens:\", tokens.shape)\n",
    "    print(\"Labels:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_gpt2.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomGPT2ForSequenceClassification(EasyTransformer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.unembed = None\n",
    "        self.classification_head = torch.nn.Linear(config.d_model * config.n_ctx, num_labels)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "       \n",
    "        embed = self.embed(tokens=input_ids)\n",
    "        embed = embed.squeeze(1)\n",
    "        #print('embed',embed.shape)\n",
    "        pos_embed = self.pos_embed(input_ids)\n",
    "        #print('pos_embed',pos_embed.shape)\n",
    "        residual = embed + pos_embed\n",
    "        #print('residual', residual.shape)\n",
    "        for block in self.blocks:\n",
    "            normalized_resid_pre = block.ln1(residual)\n",
    "            #print('normalized_resid_pre', normalized_resid_pre.shape)\n",
    "            attn_out = block.attn(normalized_resid_pre)\n",
    "            #print('attn_out', attn_out.shape)\n",
    "            resid_mid = residual + attn_out\n",
    "            #print('resid_mid', resid_mid.shape)\n",
    "\n",
    "            normalized_resid_mid = block.ln2(resid_mid)\n",
    "            #print('normalized_resid_mid', normalized_resid_mid.shape)\n",
    "            mlp_out = block.mlp(normalized_resid_mid)\n",
    "            #print('mlp_out', mlp_out.shape)\n",
    "            resid_post = resid_mid + mlp_out\n",
    "            #print('resid_post', resid_post.shape)\n",
    "        normalized_resid_final = self.ln_final(resid_post)\n",
    "        #print('normalized_resid_final', normalized_resid_final.shape)\n",
    "        normalized_resid_final = normalized_resid_final.view(normalized_resid_final.shape[0], -1)\n",
    "        #print('normalized_resid_final', normalized_resid_final.shape)\n",
    "        logits = self.classification_head(normalized_resid_final)\n",
    "        return logits\n",
    "\n",
    "# Example usage:\n",
    "config = reference_gpt2.cfg\n",
    "num_labels = 2  \n",
    "model = CustomGPT2ForSequenceClassification(config)\n",
    "\n",
    "model.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "model.float() \n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Wrap the dataloader with tqdm to add the progress bar\n",
    "    for input_ids, labels in tqdm(data_loader_train, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids = input_ids.to(device).long() \n",
    "        labels = labels.to(device).long() \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_samples += input_ids.size(0)\n",
    "        total_correct += (logits.argmax(dim=-1) == labels).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Calculate metrics\n",
    "    accuracy = total_correct / total_samples\n",
    "    average_loss = total_loss / len(data_loader_test)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Accuracy: {accuracy}\")\n",
    "    torch.save(model.state_dict(), '../trained_models/easy_transformer_gpt2small_qnli.pth')\n",
    "    \n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "total_samples = 0    \n",
    "for input_ids, labels in tqdm(data_loader_valid, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "    input_ids = input_ids.to(device).long() \n",
    "    labels = labels.to(device).long() \n",
    "    logits = model(input_ids) \n",
    "    loss = loss_fn(logits, labels)   \n",
    "    total_loss += loss.item()\n",
    "    total_samples += input_ids.size(0)\n",
    "    total_correct += (logits.argmax(dim=-1) == labels).sum().item()        \n",
    "accuracy = total_correct / total_samples\n",
    "average_loss = total_loss / len(data_loader_valid)\n",
    "print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "total_samples = 0    \n",
    "for input_ids, labels in tqdm(data_loader_test, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "    input_ids = input_ids.to(device).long() \n",
    "    labels = labels.to(device).long() \n",
    "    logits = model(input_ids) \n",
    "    loss = loss_fn(logits, labels)   \n",
    "    total_loss += loss.item()\n",
    "    total_samples += input_ids.size(0)\n",
    "    total_correct += (logits.argmax(dim=-1) == labels).sum().item()        \n",
    "accuracy = total_correct / total_samples\n",
    "average_loss = total_loss / len(data_loader_test)\n",
    "print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd9628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6403c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16838ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa14b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cee7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9362f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94ba89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
