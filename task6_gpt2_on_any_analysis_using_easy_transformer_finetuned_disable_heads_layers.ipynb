{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173f2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from easy_transformer import EasyTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from jaxtyping import Float\n",
    "from easy_transformer.hook_points import (\n",
    "    HookPoint,\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4427307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EasyTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_attn): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "reference_gpt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomGPT2ForSequenceClassification(EasyTransformer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.unembed = None\n",
    "        self.classification_head1 = torch.nn.Linear(config.d_model * config.n_ctx, num_labels)        \n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "       \n",
    "        embed = self.embed(tokens=input_ids)\n",
    "        embed = embed.squeeze(1)\n",
    "        #print('embed',embed.shape)\n",
    "        pos_embed = self.pos_embed(input_ids)\n",
    "        #print('pos_embed',pos_embed.shape)\n",
    "        residual = embed + pos_embed\n",
    "        #print('residual', residual.shape)\n",
    "        for block in self.blocks:\n",
    "            normalized_resid_pre = block.ln1(residual)\n",
    "            #print('normalized_resid_pre', normalized_resid_pre.shape)\n",
    "            attn_out = block.attn(normalized_resid_pre)\n",
    "            #print('attn_out', attn_out.shape)\n",
    "            resid_mid = residual + attn_out\n",
    "            #print('resid_mid', resid_mid.shape)\n",
    "\n",
    "            normalized_resid_mid = block.ln2(resid_mid)\n",
    "            #print('normalized_resid_mid', normalized_resid_mid.shape)\n",
    "            mlp_out = block.mlp(normalized_resid_mid)\n",
    "            #print('mlp_out', mlp_out.shape)\n",
    "            resid_post = resid_mid + mlp_out\n",
    "            #print('resid_post', resid_post.shape)\n",
    "        normalized_resid_final = self.ln_final(resid_post)\n",
    "        #print('normalized_resid_final', normalized_resid_final.shape)\n",
    "        normalized_resid_final = normalized_resid_final.view(normalized_resid_final.shape[0], -1)\n",
    "        #print('normalized_resid_final', normalized_resid_final.shape)\n",
    "        logits = self.classification_head1(normalized_resid_final)\n",
    "        return logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = load_dataset('glue', 'qqp', split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0272099",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset[120], len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589369fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_list = list(range(0, len(validation_dataset)))\n",
    "random.shuffle(integer_list)\n",
    "random_integer = random.choice(integer_list)\n",
    "print(\"Random Integer:\", random_integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71aca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_integer = 5310\n",
    "validation_dataset[random_integer]['question1']#, validation_dataset[random_integer]['sentence2'], validation_dataset[random_integer]['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(datapoint, max_length = 1024, token_to_add = 50256):\n",
    "    sep_place = []#[0]\n",
    "    sentence1 = datapoint['question1']\n",
    "    sentence1_tokens = reference_gpt2.to_tokens(sentence1, prepend_bos = False)\n",
    "    \n",
    "    sep_2 = sentence1_tokens.size(1)\n",
    "    sep_place.append(sep_2+1)\n",
    "    sentence2 = datapoint['question2']\n",
    "    sentence2_tokens = reference_gpt2.to_tokens(sentence2, prepend_bos = False)\n",
    "    \n",
    "    #token_to_add = torch.tensor([50256], dtype=torch.long)\n",
    "    #token_to_add = token_to_add.unsqueeze(0) \n",
    "    #sentence1_tokens = torch.cat((sentence1_tokens, token_to_add), dim=1)\n",
    "    concatenated_tokens = torch.cat((sentence1_tokens, sentence2_tokens), dim=1)\n",
    "    \n",
    "    labels = torch.tensor(datapoint['label'])\n",
    "    real_length = concatenated_tokens.size(1)\n",
    "    remaining_length = max_length - concatenated_tokens.size(1)\n",
    "    while remaining_length > 0:\n",
    "        concatenated_tokens = torch.cat((concatenated_tokens, torch.tensor([[token_to_add]])), dim=1)\n",
    "        remaining_length -= 1\n",
    "    return concatenated_tokens, labels, real_length, sep_place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa066d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "config = reference_gpt2.cfg\n",
    "num_labels = 2\n",
    "model = CustomGPT2ForSequenceClassification(config)\n",
    "model_path = '../trained_models/easy_transformer_gpt2small_qqp.pth' \n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1db081",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, label, length, seperators = tokenize(validation_dataset[random_integer])\n",
    "tokens, label, length, seperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d978b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattn\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.blocks[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_hooks(module):\n",
    "    def hook(module, input, output):\n",
    "        print(\"Output shape:\", output.shape)  \n",
    "    # Register the hook to the module\n",
    "    module.register_forward_hook(hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores_list = []\n",
    "def register_attention_hooks(module):\n",
    "    if isinstance(module, EasyTransformer):\n",
    "        for i, block in enumerate(module.blocks):\n",
    "            attention_module = block.attn.hook_attn\n",
    "            def hook(module, input, output):\n",
    "                # Assuming 'output' is a tuple where the first element is the attention scores\n",
    "                attention_scores = output[0]\n",
    "                attention_scores_list.append(attention_scores)\n",
    "                #print(f\"Attention scores shape for block {i}: {attention_scores.shape}\")\n",
    "                # Here you can process or store the attention scores as needed\n",
    "                # For example, you can save them to a list or perform further analysis\n",
    "\n",
    "            # Register the hook to the Attention module\n",
    "            attention_module.register_forward_hook(hook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b92c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "config = reference_gpt2.cfg\n",
    "num_labels = 2\n",
    "model = CustomGPT2ForSequenceClassification(config)\n",
    "model_path = '../trained_models/easy_transformer_gpt2small_qqp.pth' \n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model\n",
    "model.eval()\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Number of parameters in GPT-2 Small model:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokens = tokenize(validation_dataset[1], max_length = 1024, token_to_add = 50256)\n",
    "gpt2_tokens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_ablate = 5\n",
    "head_index_to_ablate = 2\n",
    "\n",
    "def head_ablation_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, 2:5, :] = 0#/1024\n",
    "    return value\n",
    "\n",
    "original_loss = model(gpt2_tokens[0])\n",
    "ablated_loss = model.run_with_hooks(\n",
    "    gpt2_tokens[0],\n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"v\", layer_to_ablate),\n",
    "        head_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "\n",
    "original_loss, ablated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a79cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a881ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  \n",
    "from transformer_lens import HookedTransformer, FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34412514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description_text = \"\"\"## Loading Models\n",
    "\n",
    "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. See my explainer for documentation of all supported models, and this table for hyper-parameters and the name used to load them. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly. \n",
    "\n",
    "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
    "loss = model(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5341fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "gpt2_tokens = model.to_tokens(gpt2_text)\n",
    "print(gpt2_tokens.device)\n",
    "gpt2_logits, gpt2_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a356f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_ablate = 0\n",
    "head_index_to_ablate = 8\n",
    "\n",
    "# We define a head ablation hook\n",
    "# The type annotations are NOT necessary, they're just a useful guide to the reader\n",
    "# \n",
    "def head_ablation_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, head_index_to_ablate, :] = 0.\n",
    "    return value\n",
    "\n",
    "original_loss = model(gpt2_tokens)\n",
    "ablated_loss = model.run_with_hooks(\n",
    "    gpt2_tokens, \n",
    "     \n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"v\", layer_to_ablate), \n",
    "        head_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "original_loss, ablated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = reference_gpt2.cfg\n",
    "num_labels = 2\n",
    "model = CustomGPT2ForSequenceClassification(config)\n",
    "model_path = '../trained_models/easy_transformer_gpt2small_qqp.pth' \n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bbf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_ablate = 0\n",
    "head_index_to_ablate = 8\n",
    "\n",
    "# We define a head ablation hook\n",
    "# The type annotations are NOT necessary, they're just a useful guide to the reader\n",
    "# \n",
    "def head_ablation_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, head_index_to_ablate, :] = 0.\n",
    "    return value\n",
    "\n",
    "original_loss = model(tokens)\n",
    "ablated_loss = model.run_with_hooks(\n",
    "    tokens, \n",
    "     \n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"v\", layer_to_ablate), \n",
    "        head_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "original_loss, ablated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112f6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
