{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f2ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from easy_transformer import EasyTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPT2ForSequenceClassification(EasyTransformer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.unembed = None\n",
    "        self.classification_head1 = torch.nn.Linear(config.d_model * config.n_ctx, num_labels)        \n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "       \n",
    "        embed = self.embed(tokens=input_ids)\n",
    "        embed = embed.squeeze(1)\n",
    "        #print('embed',embed.shape)\n",
    "        pos_embed = self.pos_embed(input_ids)\n",
    "        #print('pos_embed',pos_embed.shape)\n",
    "        residual = embed + pos_embed\n",
    "        #print('residual', residual.shape)\n",
    "        for block in self.blocks:\n",
    "            normalized_resid_pre = block.ln1(residual)\n",
    "            #print('normalized_resid_pre', normalized_resid_pre.shape)\n",
    "            attn_out = block.attn(normalized_resid_pre)\n",
    "            #print('attn_out', attn_out.shape)\n",
    "            resid_mid = residual + attn_out\n",
    "            #print('resid_mid', resid_mid.shape)\n",
    "\n",
    "            normalized_resid_mid = block.ln2(resid_mid)\n",
    "            #print('normalized_resid_mid', normalized_resid_mid.shape)\n",
    "            mlp_out = block.mlp(normalized_resid_mid)\n",
    "            #print('mlp_out', mlp_out.shape)\n",
    "            resid_post = resid_mid + mlp_out\n",
    "            #print('resid_post', resid_post.shape)\n",
    "        normalized_resid_final = self.ln_final(resid_post)\n",
    "        #print('normalized_resid_final', normalized_resid_final.shape)\n",
    "        normalized_resid_final = normalized_resid_final.view(normalized_resid_final.shape[0], -1)\n",
    "        #print('normalized_resid_final', normalized_resid_final.shape)\n",
    "        logits = self.classification_head1(normalized_resid_final)\n",
    "        return logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c660d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'stsb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = load_dataset('glue', dataset_name, split='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "subset_indices = torch.randperm(len(validation_dataset)).tolist()[:num_samples]\n",
    "validation_dataset = validation_dataset.select(subset_indices)\n",
    "len(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0272099",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset[43], len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for each in validation_dataset:\n",
    "    c.append(validation_dataset['label'])\n",
    "np.unique(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(datapoint, max_length = 1024, token_to_add = 50256):\n",
    "    sep_place = [0]\n",
    "    sentence1 = datapoint['sentence1']\n",
    "    sentence1_tokens = reference_gpt2.to_tokens(sentence1, prepend_bos = False)\n",
    "    \n",
    "    sep_2 = sentence1_tokens.size(1)\n",
    "    sep_place.append(sep_2+1)\n",
    "    sentence2 = datapoint['sentence2']\n",
    "    sentence2_tokens = reference_gpt2.to_tokens(sentence2, prepend_bos = False)\n",
    "    \n",
    "    token_to_add = torch.tensor([50264], dtype=torch.long)\n",
    "    token_to_add = token_to_add.unsqueeze(0) \n",
    "    sentence1_tokens = torch.cat((sentence1_tokens, token_to_add), dim=1)\n",
    "    concatenated_tokens = torch.cat((sentence1_tokens, sentence2_tokens), dim=1)\n",
    "    \n",
    "    recovered_tokens = reference_gpt2.to_str_tokens(concatenated_tokens)\n",
    "    \n",
    "    noun = []\n",
    "    pnoun = []\n",
    "    verb = []\n",
    "    subj = []\n",
    "    obj = []\n",
    "    neg = []\n",
    "    \n",
    "    for i, each in enumerate(recovered_tokens[:concatenated_tokens.size(1)]):\n",
    "        doc = nlp(each)\n",
    "        for token in doc:\n",
    "            \n",
    "            if token.pos_ in [\"NOUN\", \"PROPN\"]: #noun or proper noun\n",
    "                noun.append(i)\n",
    "            #if token.dep_ == \"nsubj\": #subject\n",
    "            #    subj.append(i)\n",
    "            #if token.dep_ == \"neg\": # negation\n",
    "            #    neg.append(i)\n",
    "            #if token.pos_ == \"VERB\": #verb\n",
    "            #    verb.append(i)\n",
    "            #if token.dep_ in [\"dobj\", \"iobj\"]: #object\n",
    "            #    obj.append(i)\n",
    "            if token.pos_ in [\"PRON\"]: # pronoun\n",
    "                pnoun.append(i)\n",
    "    labels = torch.tensor(datapoint['label'])\n",
    "    real_length = concatenated_tokens.size(1)\n",
    "    remaining_length = max_length - concatenated_tokens.size(1)\n",
    "    while remaining_length > 0:\n",
    "        concatenated_tokens = torch.cat((concatenated_tokens, torch.tensor([[token_to_add]])), dim=1)\n",
    "        remaining_length -= 1\n",
    "    return concatenated_tokens, labels, real_length, sep_place, noun, pnoun, verb, subj, obj, neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_hooks(module):\n",
    "    def hook(module, input, output):\n",
    "        print(\"Output shape:\", output.shape)  \n",
    "    # Register the hook to the module\n",
    "    module.register_forward_hook(hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores_list = []\n",
    "def register_attention_hooks(module):\n",
    "    if isinstance(module, EasyTransformer):\n",
    "        for i, block in enumerate(module.blocks):\n",
    "            attention_module = block.attn.hook_attn\n",
    "            def hook(module, input, output):\n",
    "                attention_scores = output[0]\n",
    "                attention_scores_list.append(attention_scores)\n",
    "            attention_module.register_forward_hook(hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c699160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(attn_scores, tosee_list, name, directory, length, layer, head):\n",
    "    temp = []\n",
    "    if len(tosee_list) == 1:\n",
    "        num = tosee_list[0] \n",
    "        attn_map = attn_scores[num, :].cpu().detach().numpy()\n",
    "        attn_map[num] = 0\n",
    "        np.save(directory + name + '_' + str(layer) + '_' + str(head) + '.npy', attn_map)\n",
    "    \n",
    "    else:\n",
    "        for num in tosee_list:\n",
    "            attn_map = attn_scores[num, :].cpu().detach().numpy()\n",
    "            attn_map[num] = 0\n",
    "            temp.append(np.sum(attn_map) / length)\n",
    "        max_value = max(temp)\n",
    "        max_value_index = temp.index(max_value)\n",
    "        attn_map = attn_scores[max_value_index, :].cpu().detach().numpy()\n",
    "        attn_map[max_value_index] = 0\n",
    "        np.save(directory + name + '_' + str(layer) + '_' + str(head) + '.npy', attn_map)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ce09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "model_path = '../trained_models/easy_transformer_gpt2small_' + dataset_name + '.pth' \n",
    "  \n",
    "for i, point in enumerate(validation_dataset): \n",
    "    temp = []\n",
    "    print('data point ',i)\n",
    "    reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "    config = reference_gpt2.cfg\n",
    "    model = CustomGPT2ForSequenceClassification(config)\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    device = torch.device(\"mps\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    reference_gpt2.to(device)\n",
    "    reference_gpt2.eval()\n",
    "    \n",
    "    tokens, label, length, seperators, noun, pnoun, verb, subj, obj, neg = tokenize(point)\n",
    "    \n",
    "    attention_scores_list = []\n",
    "    register_attention_hooks(model)\n",
    "    outputs = model(tokens)\n",
    "    \n",
    "    for attention in range(0, 12):\n",
    "        for head in range(0, 12):\n",
    "            directory = '../gpt2_small/verb_subject/' + dataset_name + '/' + str(i) + '/'\n",
    "            if len(noun) > 0 and len(pnoun) > 0: #or len(noun) > 0 or len(pnoun) > 0 or len(obj) > 0 or len(noun) > 0:\n",
    "                if os.path.exists(directory):\n",
    "                    pass\n",
    "                else:\n",
    "                    os.mkdir(directory)\n",
    "                \n",
    "                get_max(attention_scores_list[attention][head,:,:], noun, 'noun', directory, length, attention, head)\n",
    "                get_max(attention_scores_list[attention][head,:,:], pnoun, 'pnoun', directory, length, attention, head)\n",
    "                \n",
    "                #get_max(attention_scores_list[attention][head,:,:], verb, 'verb', directory, length, attention, head)\n",
    "                #get_max(attention_scores_list[attention][head,:,:], subj, 'subj',  directory, length, attention, head)\n",
    "                #get_max(attention_scores_list[attention][head,:,:], obj, 'obj', directory, length, attention, head)\n",
    "                #get_max(attention_scores_list[attention][head,:,:], neg, 'neg', directory, length, attention, head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315eee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'stsb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa9226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '../gpt2_small/verb_subject/' + dataset_name + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a79cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = os.listdir(main_path)\n",
    "if '.DS_Store' in all_points:\n",
    "    all_points.remove('.DS_Store')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = ['noun', 'pnoun', 'verb', 'subj', 'obj', 'neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a881ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict = {}\n",
    "for relation in relations:\n",
    "    print(relation)\n",
    "    new_array = empty_array = np.empty((12, 12))\n",
    "    for layer in range(0, 12):\n",
    "        temp = []\n",
    "        start = 0\n",
    "        for head in range(0, 12):\n",
    "            for folder in all_points:\n",
    "                files = os.listdir(main_path + folder + '/')\n",
    "                if '.DS_Store' in files:\n",
    "                    files.remove('.DS_Store')\n",
    "                check_file = relation + '_' + str(layer) + '_' + str(head) + '.npy'\n",
    "                if check_file in files:\n",
    "                    temp.append(np.load(main_path + folder + '/' + check_file).sum())  \n",
    "            mean = np.mean(np.array(temp))\n",
    "            new_array[layer, head] = mean\n",
    "    relation_dict[relation] = new_array\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf64466",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dict['noun'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in relation_dict.keys():\n",
    "    uni = np.unique(relation_dict[key])\n",
    "    if np.isnan(relation_dict[key]).any() != True:\n",
    "        plt.imshow(relation_dict[key], cmap='YlGn') \n",
    "        plt.colorbar()\n",
    "        plt.savefig('../gpt2_small/verb_subject/' + dataset_name + '_' + key + '_finetuned' + '.png')\n",
    "\n",
    "        plt.show()\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7ae26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95040d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
