# Replicating-Revealing-the-Dark-Secrets-of-BERT-on-GPT2-small

Task1: Self-attention patterns
Task2: Relation-specific heads in GPT2
Task3: Change in self-attention patterns after fine-tuning
Task4: Attention to linguistic features
Task5: Token-to-token attention
task6: Disabling self-attention heads

The results of the experiments are outlined in '9_61_Pick_a_BERTology_paper_and_try_to_replicate_it_on_GPT2small', along with a comprehensive description of the methodology employed.
