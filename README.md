# Replicating-Revealing-the-Dark-Secrets-of-BERT-on-GPT2-small

Task1: Self-attention patterns </br>
Task2: Relation-specific heads in GPT2</br>
Task3: Change in self-attention patterns after fine-tuning</br>
Task4: Attention to linguistic features</br>
Task5: Token-to-token attention</br>
task6: Disabling self-attention heads</br>

The results of the experiments are outlined in ['9_61_Pick_a_BERTology_paper_and_try_to_replicate_it_on_GPT2small'](https://github.com/Varchita-Beena/Replicating-Revealing-the-Dark-Secrets-of-BERT-on-GPT2-small/tree/main), along with a comprehensive description of the methodology employed.
